# Gemma2-Application-GenAI

Sure! Here's a complete and clean `README.md` file for your project that explains what it does, how it works locally with a lightweight model (Gemma), and how to set it up:

---

````markdown
# 🧠 LangChain Demo with Gemma Model (Ollama + Streamlit)

This is a lightweight Generative AI chatbot built using **LangChain**, **Gemma 2B model**, **Ollama**, and **Streamlit**. It runs **entirely on your local machine**, making it ideal for developers with **limited computing resources** who want to experiment with LLMs **without using cloud-based models**.

---

## 🚀 What is this Project?

This app allows you to:
- Ask questions via a simple **Streamlit UI**
- Get responses generated by the **Gemma 2B model**, served through **Ollama**
- Use **LangChain** to structure the prompt and parse responses
- Run **locally without needing an OpenAI or cloud-based GPU**

---

## 🧠 About the Model: Gemma 2B

[Gemma](https://ai.google.dev/gemma) is a family of lightweight, open models from Google.  
This project uses the **Gemma 2B** variant, which:
- Is designed for running on CPUs or low-end GPUs (even with 8GB–16GB RAM)
- Is optimized for **speed and efficiency**
- Provides decent responses for general-purpose use cases

We use the model via **Ollama**, which handles model serving easily on your machine.

---

## 📦 Tech Stack

- `LangChain`: Prompt handling and chaining
- `Streamlit`: Interactive frontend
- `Ollama`: Local model serving (Gemma 2B)
- `Python`: Core development language

---

## 🖥️ Prerequisites

- Python 3.9+
- [Ollama](https://ollama.com/) installed and running
- Gemma 2B model pulled locally:
  ```bash
  ollama pull gemma:2b
````

---

## ⚙️ Installation & Setup

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/KartikNimhan/Gemma2-Application-GenAI-.git
   cd Gemma2-Application-GenAI-
   ```

2. **Create Virtual Environment** (optional but recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate     # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama (in another terminal)**:

   ```bash
   ollama serve
   ollama run gemma:2b
   ```

5. **Run the Streamlit App**:

   ```bash
   streamlit run app.py
   ```

---

## 🌐 Environment Variables

Create a `.env` file with the following (if using LangSmith tracking):

```env
OPENAI_API_KEY=your_dummy_key
LANGCHAIN_API_KEY=your_langchain_api_key
LANGCHAIN_PROJECT=GemmaChat
```

> ℹ️ You don’t need a real OpenAI key unless you're integrating OpenAI-based models. Ollama runs locally.

---

## ✅ Features

* 💬 Ask any question and get AI-generated answers
* 🧱 Modular LangChain architecture
* ⚡ Fast and local: No cloud APIs required
* 🖼️ Clean Streamlit UI for interaction

---

## 📷 Screenshot

![App Screenshot](https://user-images.githubusercontent.com/placeholder/app_screenshot.png)

---

## 🧪 Example Questions

* What is Generative AI?
* How does LangChain work?
* Who is Alan Turing?

---

## 🛠️ Future Improvements

* Add PDF/document ingestion
* Add retrieval-based QA with vector store
* Add model selector (Gemma, LLaMA, Mistral, etc.)

---

## 📜 License

This project is open-source and free to use under the [MIT License](LICENSE).

---

## 🙋‍♂️ Author

**Kartik Nimhan**
ML & GenAI Enthusiast | [GitHub](https://github.com/KartikNimhan)

---

```

```
