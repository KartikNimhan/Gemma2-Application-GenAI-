# Gemma2-Application-GenAI

Sure! Here's a complete and clean `README.md` file for your project that explains what it does, how it works locally with a lightweight model (Gemma), and how to set it up:

---

````markdown
# ğŸ§  LangChain Chatbot using Gemma 2B (Ollama + Streamlit)

A lightweight, local-first **Generative AI chatbot** powered by [Googleâ€™s Gemma 2B model](https://ai.google.dev/gemma), built using **LangChain**, **Ollama**, and **Streamlit**.

> âš¡ï¸ Perfect for developers with limited computing resources â€“ no OpenAI key or cloud GPU required!

---

## ğŸš€ Features

âœ… Ask any question through a simple **Streamlit UI**  
âœ… Get AI responses generated **locally** via the **Gemma 2B** model  
âœ… Powered by **LangChain** for prompt orchestration  
âœ… Works even on low-end systems (8â€“16 GB RAM)  
âœ… Runs entirely offline after setup

---

## ğŸ§  About Gemma 2B

[Gemma](https://ai.google.dev/gemma) is a family of open, lightweight models released by Google.

We use the **Gemma 2B** model because:
- It's optimized for **CPU and low-end GPU** environments.
- Itâ€™s efficient and fast for real-time queries.
- It can be run locally using **Ollama** with just one command.

---

## ğŸ§° Tech Stack

| Tool         | Purpose                        |
|--------------|--------------------------------|
| ğŸ¦œ LangChain  | Prompt management & chaining   |
| ğŸ§  Gemma 2B   | Lightweight LLM (via Ollama)   |
| ğŸ“¦ Ollama     | Local model inference engine   |
| ğŸŒ Streamlit  | Frontend for chat interface    |
| ğŸ Python     | Core language (3.9+)           |

---

## ğŸ–¥ï¸ Prerequisites

Make sure you have:

- **Python 3.9+** installed
- **Ollama** installed â†’ [https://ollama.com](https://ollama.com)
- The **Gemma 2B model** pulled locally:

```bash
ollama pull gemma:2b


---

## âš™ï¸ Installation & Setup

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/KartikNimhan/Gemma2-Application-GenAI-.git
   cd Gemma2-Application-GenAI-
   ```

2. **Create Virtual Environment** (optional but recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate     # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama (in another terminal)**:

   ```bash
   ollama serve
   ollama run gemma:2b
   ```

5. **Run the Streamlit App**:

   ```bash
   streamlit run app.py
   ```

---

## ğŸŒ Environment Variables

Create a `.env` file with the following (if using LangSmith tracking):

```env
OPENAI_API_KEY=your_dummy_key
LANGCHAIN_API_KEY=your_langchain_api_key
LANGCHAIN_PROJECT=GemmaChat
```

> â„¹ï¸ You donâ€™t need a real OpenAI key unless you're integrating OpenAI-based models. Ollama runs locally.

---

## âœ… Features

* ğŸ’¬ Ask any question and get AI-generated answers
* ğŸ§± Modular LangChain architecture
* âš¡ Fast and local: No cloud APIs required
* ğŸ–¼ï¸ Clean Streamlit UI for interaction

---

## ğŸ“· Screenshot

![App Screenshot](https://user-images.githubusercontent.com/placeholder/app_screenshot.png)

---

## ğŸ§ª Example Questions

* What is Generative AI?
* How does LangChain work?
* Who is Alan Turing?

---

## ğŸ› ï¸ Future Improvements

* Add PDF/document ingestion
* Add retrieval-based QA with vector store
* Add model selector (Gemma, LLaMA, Mistral, etc.)

---

## ğŸ“œ License

This project is open-source and free to use under the [MIT License](LICENSE).

---

## ğŸ™‹â€â™‚ï¸ Author

**Kartik Nimhan**
ML & GenAI Enthusiast | [GitHub](https://github.com/KartikNimhan)

---

```

```
